# %%
!pip
install - q
neo4j
# %%
from neo4j import GraphDatabase
import json

# Configuration d'apr√®s tes infos
URI = "bolt://127.0.0.1:7687"
USER = "neo4j"
PASSWORD = "12345678"
DATABASE = "projet"
JSON_PATH = r"C:\ProjetAI\legal_chunks.json"  #


def remplir_base_graphe():
    # 1. Charger les donn√©es JSON
    with open(JSON_PATH, "r", encoding="utf-8") as f:
        chunks = json.load(f)

    # 2. Se connecter au driver
    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))

    with driver.session(database=DATABASE) as session:
        print(f"üöÄ Injection de {len(chunks)} articles dans la base '{DATABASE}'...")

        # Requ√™te Cypher pour cr√©er les n≈ìuds Source et Article
        query = """
        UNWIND $data AS item
        MERGE (s:Source {name: item.source})
        CREATE (a:Article {
            article_num: item.article,
            content: item.text
        })
        CREATE (a)-[:APPARTIENT_A]->(s)
        """
        session.run(query, data=chunks)

    driver.close()
    print("‚úÖ Migration termin√©e avec succ√®s !")


# Ex√©cuter la fonction
remplir_base_graphe()
# %%
import re
from neo4j import GraphDatabase

# Configuration (identique √† ton √©tape pr√©c√©dente)
URI = "bolt://127.0.0.1:7687"
USER = "neo4j"
PASSWORD = "12345678"
DATABASE = "projet"


def creer_citations_automatiques():
    driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))

    with driver.session(database=DATABASE) as session:
        print("üîç Analyse du texte pour trouver des citations...")

        # 1. On r√©cup√®re tous les articles
        articles = session.run("MATCH (a:Article) RETURN a.article_num as num, id(a) as id, a.content as text")

        for record in articles:
            texte = record["text"]
            id_source = record["id"]

            # 2. On cherche des motifs comme "Article 45" ou "art. 45"
            # Ce pattern est adapt√© au format des textes juridiques marocains
            citations = re.findall(r"(?:Article|art\.)\s+(\d+)", texte, re.IGNORECASE)

            for num_cite in citations:
                # 3. On cr√©e un lien vers l'article mentionn√© s'il existe dans la m√™me source
                session.run("""
                    MATCH (a1) WHERE id(a1) = $id_source
                    MATCH (a2:Article {article_num: $num_cite})
                    WHERE a1 <> a2
                    MERGE (a1)-[:CITE]->(a2)
                """, id_source=id_source, num_cite=num_cite)

    driver.close()
    print("‚úÖ Liens de citations cr√©√©s dans le graphe !")


# Lancer la d√©tection
creer_citations_automatiques()
# %%
# 0) Installer les d√©pendances (√† relancer apr√®s chaque changement de runtime)
!pip
install - q
faiss - cpu
sentence - transformers
transformers
accelerate

import os, json, faiss, numpy as np
from sentence_transformers import SentenceTransformer

# %%
import os

BASE_DIR = r"C:\ProjetAI"
print("BASE_DIR existe ?", os.path.exists(BASE_DIR))
print("Contenu de BASE_DIR :", os.listdir(BASE_DIR))

# %%
# 2) Indiquer le dossier o√π se trouvent les fichiers sur Drive
# Exemple : "/content/drive/MyDrive/legal_bot_db"
DRIVE_DB_PATH = "C:\ProjetAI"  # <-- change ce chemin si besoin

index_path = f"{DRIVE_DB_PATH}/legal_index.faiss"
chunks_path = f"{DRIVE_DB_PATH}/legal_chunks.json"

print("Index path :", index_path)
print("Chunks path:", chunks_path)

# V√©rification du dossier
!ls - lh
"$DRIVE_DB_PATH"

# %%
# 3) Charger l'embedder (le m√™me que pour l'indexation)
embedder = SentenceTransformer("intfloat/multilingual-e5-base")

# %%
# 4) Charger l'index FAISS + les chunks depuis Drive
index = faiss.read_index(index_path)

with open(chunks_path, "r", encoding="utf-8") as f:
    all_chunks = json.load(f)

print("‚úÖ Index charg√© | nb chunks:", len(all_chunks))
print("‚úÖ Sources dispo:", sorted(set(c.get("source") for c in all_chunks if c.get("source"))))


# %% md
## üîé Retrieval (recherche s√©mantique)
# %%
def retrieve(question, k=5, source=None, min_score=0.25):
    """Retourne top-k chunks pertinents.
    - source: filtrer un code particulier (ex: 'droit_penal')
    - min_score: seuil pour √©viter hallucinations
    """
    q_emb = embedder.encode([f"query: {question}"], normalize_embeddings=True)
    q_emb = np.array(q_emb).astype("float32")

    k_search = k * 5 if source else k
    scores, idxs = index.search(q_emb, k_search)

    results = []
    for i, score in zip(idxs[0], scores[0]):
        if score < min_score:
            continue
        chunk = all_chunks[i]
        if (source is None) or (chunk.get("source") == source):
            results.append((chunk, float(score)))
        if len(results) >= k:
            break
    return results


# %% md
## ü§ñ Charger un LLM Hugging Face
Choisis
un
mod√®le
instruct.Si
tu
es
sur
CPU, prends
un
mod√®le
l√©ger.
Exemples
CPU - friendly: `Qwen / Qwen2
.5 - 1.5
B - Instruct
`, `microsoft / phi - 2`.
Ici
on
met
Mistral
7
B
Instruct(GPU
recommand√©).
# %%
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map=None,  # CPU
    torch_dtype=torch.float32  # CPU
)

gen = pipeline("text-generation", model=model, tokenizer=tokenizer)
gen.tokenizer.pad_token_id = gen.tokenizer.eos_token_id

print(gen("Question: Quelle est la peine du vol ?\nR√©ponse:", max_new_tokens=150)[0]["generated_text"])


# %% md
## üß† RAG Answer
# %%
def get_graph_context(retrieved_chunks):
    """Pour chaque chunk trouv√© par FAISS, on va chercher ses citations dans Neo4j."""
    driver = GraphDatabase.driver("bolt://127.0.0.1:7687", auth=("neo4j", "12345678"))
    extra_chunks = []

    # On m√©morise les articles d√©j√† pr√©sents pour √©viter les doublons
    seen_articles = {str(c.get('article')) for c, score in retrieved_chunks}

    with driver.session(database="projet") as session:
        for chunk, score in retrieved_chunks:
            art_num = str(chunk.get('article'))

            # Requ√™te Cypher : trouve les articles cit√©s par l'article actuel
            result = session.run("""
                MATCH (a:Article {article_num: $num})-[:CITE]->(cite:Article)
                RETURN cite.article_num as num, cite.content as text
            """, num=art_num)

            for record in result:
                if record["num"] not in seen_articles:
                    # On cr√©e un faux chunk pour l'article cit√©
                    extra_chunks.append({
                        'source': 'R√©f√©rence li√©e (Graphe)',
                        'article': record["num"],
                        'text': record["text"]
                    })
                    seen_articles.add(record["num"])

    driver.close()
    return extra_chunks


# %%
def rag_answer_v2(question, k=5, source=None, min_score=0.25, max_ctx_chars=6000):
    # 1. Recherche initiale avec FAISS (votre fonction actuelle)
    retrieved = retrieve(question, k=k, source=source, min_score=min_score)

    if not retrieved:
        return "Je ne sais pas. Aucun article pertinent trouv√©.", []

    # 2. EXTENSION GRAPHE : On va chercher les citations dans Neo4j
    extra_context = get_graph_context(retrieved)

    # 3. Construction du contexte enrichi
    parts = []
    total = 0

    # On ajoute d'abord les articles principaux (FAISS)
    for c, score in retrieved:
        chunk = f"[{c['source']} | Article {c.get('article')} | score={score:.2f}] {c['text']}"
        if total + len(chunk) < max_ctx_chars:
            parts.append(chunk)
            total += len(chunk)

    # On ajoute ensuite les articles li√©s par le graphe (Neo4j)
    for c in extra_context:
        chunk = f"[LIEN | Article {c.get('article')}] {c['text']}"
        if total + len(chunk) < max_ctx_chars:
            parts.append(chunk)
            total += len(chunk)

    context = "\n\n".join(parts)

    # 4. Prompt et G√©n√©ration avec TinyLlama
    prompt = f"""
Tu es un assistant juridique marocain.
R√©ponds √† la QUESTION en utilisant le CONTEXTE (articles principaux et li√©s).

CONTEXTE:
{context}

QUESTION:
{question}

R√âPONSE:
"""

    out = gen(prompt, max_new_tokens=450, do_sample=False)[0]["generated_text"]
    answer = out.split("R√âPONSE")[-1].strip()

    return answer, retrieved + extra_context


# %% md
## ‚úÖ Tests rapides
# %%
# 1. D√©finition de la question de test
question_test = "Quelle est la proc√©dure √† suivre pour un licenciement pour faute grave ?"

# 2. Ex√©cution du GraphRAG
print("üîç Recherche en cours (FAISS + Neo4j)...")
reponse, sources = rag_answer_v2(question_test, k=4)

# 3. Affichage de la r√©ponse
print("\n" + "=" * 50)
print("ü§ñ R√âPONSE DU LEGALBOT :")
print("=" * 50)
print(reponse)

print("\n" + "=" * 50)
print("üìö SOURCES UTILIS√âES :")
print("=" * 50)
for i, s in enumerate(sources):
    # On distingue les sources FAISS des sources Graph
    type_source = "Graphe (Lien)" if isinstance(s, dict) and s.get(
        'source') == 'R√©f√©rence li√©e (Graphe)' else "FAISS (Similarit√©)"

    # Extraction propre des infos selon le type
    if isinstance(s, tuple):  # Format FAISS: (chunk_dict, score)
        chunk, score = s
        print(f"{i + 1}. [{type_source}] Article {chunk.get('article')} (Score: {score:.2f})")
    else:  # Format Graphe
        print(f"{i + 1}. [{type_source}] Article {s.get('article')}")
# %%
!pip
install - q
gradio
# %%
import gradio as gr


def bot_interface(question, top_k, temperature):
    """Fonction de pont entre l'interface et votre code GraphRAG."""

    # 1. Appel de votre fonction RAG avec les param√®tres de l'interface
    # On modifie temporairement rag_answer_v2 pour accepter la temp√©rature
    answer, sources = rag_answer_v2_ui(question, k=int(top_k), temp=float(temperature))

    # 2. Formatage des sources pour l'affichage
    sources_text = "\n\n".join([
        f"üìå {s[0]['source']} (Art. {s[0].get('article')})" if isinstance(s, tuple)
        else f"üîó {s.get('source')} (Art. {s.get('article')})"
        for s in sources
    ])

    return answer, sources_text


def rag_answer_v2_ui(question, k=5, temp=0.1):
    """Version adapt√©e pour l'interface avec gestion de la temp√©rature."""
    retrieved = retrieve(question, k=k)  # Utilise votre fonction FAISS

    if not retrieved:
        return "Aucun article trouv√©.", []

    extra_context = get_graph_context(retrieved)  # Utilise Neo4j

    # Construction du contexte (identique √† votre version pr√©c√©dente)
    parts = []
    for c, score in retrieved:
        parts.append(f"[{c['source']} | Art. {c.get('article')}] {c['text']}")
    for c in extra_context:
        parts.append(f"[LIEN | Art. {c.get('article')}] {c['text']}")

    context = "\n\n".join(parts)
    prompt = f"CONTEXTE:\n{context}\n\nQUESTION:\n{question}\n\nR√âPONSE:"

    # G√©n√©ration avec prise en compte de la temp√©rature
    # do_sample=True est n√©cessaire pour utiliser la temp√©rature
    out = gen(prompt, max_new_tokens=450, do_sample=True, temperature=temp)[0]["generated_text"]
    answer = out.split("R√âPONSE")[-1].strip()

    return answer, retrieved + extra_context


# %%
# Configuration de l'interface visuelle
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üá≤üá¶ LegalBot - Assistant Juridique GraphRAG")
    gr.Markdown("Posez vos questions sur le droit marocain (Code du Travail, Commerce, etc.)")

    with gr.Row():
        with gr.Column(scale=1):
            # Param√®tres de contr√¥le
            input_text = gr.Textbox(label="Votre question", placeholder="Ex: Quelle est la dur√©e du pr√©avis ?")
            slider_k = gr.Slider(minimum=1, maximum=15, value=5, step=1, label="Top-K (Articles FAISS)")
            slider_temp = gr.Slider(minimum=0.1, maximum=1.0, value=0.1, step=0.1, label="Temp√©rature (Cr√©ativit√©)")
            btn = gr.Button("Rechercher üîç", variant="primary")

        with gr.Column(scale=2):
            # Affichage des r√©sultats
            output_answer = gr.Textbox(label="R√©ponse du LegalBot", lines=10)
            output_sources = gr.Textbox(label="Sources juridiques consult√©es (FAISS + Neo4j)", lines=5)

    # Action du bouton
    btn.click(fn=bot_interface, inputs=[input_text, slider_k, slider_temp], outputs=[output_answer, output_sources])

# Lancement local (accessible sur http://127.0.0.1:7860)
demo.launch(share=False)